{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0RPss8Ev7oAb9j9GE3O21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bruhseriously/AIR/blob/main/LightGBM_Llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-neo"
      ],
      "metadata": {
        "id": "OHRXDIUbGiX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDC2YF88EobD"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "0U1zYt7vGe5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
      ],
      "metadata": {
        "id": "ujl-PB3HGgkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_text):\n",
        "    \"\"\"\n",
        "    Simulate GPT processing the input text and extracting structured data\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "KlKdN0xLGg9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the GPT output (usually this would be parsing the response, but here it's mocked)\n",
        "guest_request = \"Please delay vessel XYZ by 2 hours due to a storm.\"\n",
        "print(f\"Guest Request: {guest_request}\")\n",
        "\n",
        "structured_data = {\n",
        "    'vessel_id': 'XYZ',\n",
        "    'delay_hours': 2,\n",
        "    'priority_reason': 1,  # High priority due to storm\n",
        "    'customer_importance': 3  # High customer importance\n",
        "}\n",
        "\n",
        "print(f\"Structured Data Extracted: {structured_data}\")"
      ],
      "metadata": {
        "id": "tjXDA-ZDGtzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a simple LightGBM model to rank vessels (you can imagine this would be pre-trained and stored)\n",
        "# Simulating some simple training data for LightGBM\n",
        "data = {\n",
        "    'vessel_id': [1, 2, 3, 4, 5],\n",
        "    'delay_hours': [2, 5, 1, 3, 4],\n",
        "    'priority_reason': [0, 1, 0, 1, 1],  # 0: Low priority, 1: High priority\n",
        "    'customer_importance': [3, 2, 1, 3, 2]  # High customer importance for some\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target (let's assume the target is the priority, 1 is high priority, 0 is low priority)\n",
        "X = df[['delay_hours', 'priority_reason', 'customer_importance']]\n",
        "y = np.array([1, 0, 1, 0, 1])  # Target labels (1: high priority, 0: low priority)\n",
        "\n",
        "# Train LightGBM model\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error'\n",
        "}\n",
        "model_lgb = lgb.train(params, train_data, 100)\n",
        "\n",
        "# 4. Rank the incoming vessel based on the structured data (delay, priority, customer importance)\n",
        "X_input = np.array([[structured_data['delay_hours'],\n",
        "                     structured_data['priority_reason'],\n",
        "                     structured_data['customer_importance']]])\n",
        "\n",
        "# Predict the priority (higher value = higher priority)\n",
        "priority = model_lgb.predict(X_input)\n",
        "print(f\"LightGBM Model Priority Score for Vessel {structured_data['vessel_id']}: {priority[0]}\")"
      ],
      "metadata": {
        "id": "iK_KvDfWE11v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Simulate rescheduling decision based on LightGBM's priority\n",
        "if priority > 0.5:\n",
        "    reschedule_message = f\"Vessel {structured_data['vessel_id']} is moved to higher priority due to the storm and will be delayed by {structured_data['delay_hours']} hours.\"\n",
        "else:\n",
        "    reschedule_message = f\"Vessel {structured_data['vessel_id']} remains on the current schedule.\"\n",
        "\n",
        "# 6. Output the final decision (rescheduling)\n",
        "print(f\"Rescheduling Decision: {reschedule_message}\")\n"
      ],
      "metadata": {
        "id": "L_M2SV4WG-F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama"
      ],
      "metadata": {
        "id": "Lr2Jt6aFHFLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece #library for text tokenization\n",
        "!pip install -U bitsandbytes #library for loading and managing models\n",
        "!pip install -q transformers torch accelerate #libraries for working with HF models and Pytorch\n",
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig\n",
        "from threading import Thread\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2X9mt72xPk0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLaMA Model and Tokenizer\n",
        "model_id = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
        "device = \"cuda\"  # Use GPU for faster processing\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "def generate_llama_response(input_text, max_new_tokens=4096):\n",
        "    \"\"\"\n",
        "    Function to use LLaMA model to process text and extract actionable data\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example guest request\n",
        "guest_request = \"Please delay vessel XYZ by 2 hours due to a storm.\"\n",
        "print(f\"Guest Request: {guest_request}\")\n",
        "\n",
        "# Generate LLaMA response (e.g., extracting relevant data)\n",
        "llama_response = generate_llama_response(guest_request)\n",
        "print(f\"LLaMA Response: {llama_response}\")"
      ],
      "metadata": {
        "id": "KCMgvRAVGQdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated structured data based on LLaMA response (you'd normally parse the response)\n",
        "structured_data = {\n",
        "    'vessel_id': 'XYZ',\n",
        "    'delay_hours': 2,\n",
        "    'priority_reason': 1,  # High priority due to storm\n",
        "    'customer_importance': 3  # High customer importance\n",
        "}\n",
        "\n",
        "# Simulating a simple LightGBM model\n",
        "data = {\n",
        "    'vessel_id': [1, 2, 3, 4, 5],\n",
        "    'delay_hours': [2, 5, 1, 3, 4],\n",
        "    'priority_reason': [0, 1, 0, 1, 1],  # 0: Low priority, 1: High priority\n",
        "    'customer_importance': [3, 2, 1, 3, 2]  # High customer importance for some\n",
        "}\n",
        "\n",
        "# Training LightGBM Model\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "X = df[['delay_hours', 'priority_reason', 'customer_importance']]\n",
        "y = np.array([1, 0, 1, 0, 1])  # 1: high priority, 0: low priority\n",
        "\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error'\n",
        "}\n",
        "lgb_model = lgb.train(params, train_data, 100)\n",
        "\n",
        "# Predict the priority of the incoming request using LightGBM\n",
        "X_input = np.array([[structured_data['delay_hours'],\n",
        "                     structured_data['priority_reason'],\n",
        "                     structured_data['customer_importance']]])\n",
        "\n",
        "priority = lgb_model.predict(X_input)\n",
        "print(f\"LightGBM Priority Score: {priority[0]}\")"
      ],
      "metadata": {
        "id": "IeKE5pgDPtV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate rescheduling decision\n",
        "if priority > 0.5:\n",
        "    reschedule_message = f\"Vessel {structured_data['vessel_id']} moved to higher priority.\"\n",
        "else:\n",
        "    reschedule_message = f\"Vessel {structured_data['vessel_id']} remains on current schedule.\"\n",
        "\n",
        "print(f\"Rescheduling Decision: {reschedule_message}\")"
      ],
      "metadata": {
        "id": "-66S_y9tQoA-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}